# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02a_model.ipynb (unless otherwise specified).

__all__ = ['get_cuda', 'get_cpu', 'get_mnetV2_feature_net', 'get_feats_dataloaders', 'get_all_feats0', 'get_all_feats',
           'save_featsXsize', 'collate_featsXsize', 'load_and_prep_sneaker', 'get_mnet_feature', 'query_neighs',
           'plot_sneak_neighs', 'get_umap_embedding', 'on_click_find_similar', 'btn_run', 'out_pl', 'lbl_neighs',
           'btn_upload']

# Cell
from .imports import *
from .core import *
from .data import *

from ipywidgets import widgets
from sklearn.neighbors import NearestNeighbors
import umap
import seaborn as sns
from sklearn.decomposition import PCA

#!conda install -c conda-forge umap-learn

# Cell

# these are pure torch wrappers... should replace with fastai calls.
def get_cuda():
    """ try to load onto GPU"""
    return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def get_cpu():
    """
    set device to cpu.
    """
    return torch.device("cpu")

# Cell

def get_mnetV2_feature_net(to_cuda=False):
    # use fastai builtin Identity
    #     ## use an "identity" layer to replace 1000 category classifier
    #     class Identity(nn.Module):
    #         def __init__(self):
    #             super(Identity, self).__init__()

    #         def forward(self, x):
    #             return x

    mnetv2  = torchvision.models.mobilenet_v2(pretrained=True)
    mnetv2.classifier = Identity()

    if to_cuda:
        device = get_cuda()
    else:
        device = get_cpu()

    mnetv2 = mnetv2.to(device)
    mnetv2.eval()  # force it to eval mode to turn off batchnorm/dropout

    # just incase we forget the no_grad()
    for param in mnetv2.parameters():
        param.requires_grad = False

    return mnetv2

# Cell

def get_feats_dataloaders(data,batch_size, size, device):
    # put everythign in train, and don't do any augmentation since we are just going
    # resize to 224
    # set up the helper functions to pass data into the
    def get_x(r): return path_images/r['path']
    #def get_y(r): return r['Category']  # we aren't actually using the category here (see 02_model.ipynb)
    def get_fname(r): return r['path']

    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
                   splitter=IndexSplitter([]),
                   get_x=get_x,
                   get_y=get_fname,
                   item_tfms=Resize(size, method='pad', pad_mode='border'),
                   batch_tfms=Normalize.from_stats(*imagenet_stats))  # border pads white...
    dls = dblock.dataloaders(data,bs=batch_size,drop_last=False,device=device)
    #since we are just calculating the features for all the data turn off shuffling
    dls.train.shuffle=False
    return dls


# Cell


def get_all_feats0(dls,model,to_df=False):
    " expect dls to give us sorted (alphabetical)"
    outs = []
    clss = []
    paths = []
    batchn = 0


    with torch.no_grad():
        for imgs,classes in dls.train:
            outs.extend(conv_net(imgs))
            clss.extend(classes)
            paths.extend( [dls[0].vocab[c] for c in classes])
            batchn += 1

    #inps = torch.stack(inps)
    if to_df:
        out = np.stack(to_detach(outs))
        cs = np.stack(to_detach(clss))
        fn = np.stack(paths)
        return pd.DataFrame({"path": fn, "classes":cs, "features":out})

    return outs

def get_all_feats(dls,conv_net):
    outs = []
    clss = []
    paths = []
    batchn = 0


    with torch.no_grad():
        for imgs,classes in dls.train:
            outs.extend(conv_net(imgs))
            clss.extend(classes)
            paths.extend( [dls[0].vocab[c] for c in classes])
            batchn += 1

    out = [np.array(to_detach(o)) for o in outs] #np.stack(to_detach(outs))
    cs = np.stack(to_detach(clss))
    fn = np.stack(paths)

    #out.shape,cs.shape,fn.shape

    #store all relevant info in a pandas datafram
    df_feats = pd.DataFrame({"path": fn, "classes":cs, "features":out})
    return df_feats





# Cell
def save_featsXsize(im_sizes = IMG_SIZES):
    for i,sz in enumerate(im_sizes):
        print(IMG_SIZES[sz])
        device = get_cuda()
        dls = get_feats_dataloaders(df,batch_size,IMG_SIZES[sz],device)
        model = get_mnetV2_feature_net(to_cuda=True)
        df_f = get_all_feats(dls,model)
        # save it
        filename = f"mobilenetv2-features_{sz}"
        df_f.to_pickle(f"data/{filename}.pkl")



# Cell
def collate_featsXsize(df,dump=True):
    """
    merge the features from small/med/large
    """
    filename = f"mobilenetv2-features_small"
    df_sm = pd.read_pickle(f"data/{filename}.pkl")
    filename = f"mobilenetv2-features_medium"
    df_md = pd.read_pickle(f"data/{filename}.pkl")
    filename = f"mobilenetv2-features_large"
    df_lg = pd.read_pickle(f"data/{filename}.pkl")


    df_test = pd.merge(df_sm,df_md,how='left',on='path',suffixes=('_sm','_md'))
    df_test = pd.merge(df_test,df_lg,how='left',on='path')
    df_test = df_test.rename(columns={"classes": "classes_lg", "features": "features_lg"})


    #df2 = df.merge(df_feat)
    # explicitly:
    df2 = pd.merge(df, df_test,  how='left', on='path')

    # save it
    if dump:
        filename = "zappos-50k-mobilenetv2-features_"
        df2.to_pickle(f"data/{filename}.pkl")

    df2 = df2.sort_values('path', ascending=True)
    df2 = df2.reset_index(drop=True)
    if dump:
        filename = "zappos-50k-mobilenetv2-features_sort_3"
        df2.to_pickle(f"data/{filename}.pkl")

    return df2

# Cell
def load_and_prep_sneaker(image_path,size=IMG_SIZE,to_cuda=False):
    """input: expects a Path(), but string should work

        output TensorImage ready to unsqueeze and "embed"
    TODO:  make this a Pipeline?

    """
    base_im = PILImage.create(image_path)
    #BUG: pass split_idx=1 to avoid funny business
    img = Resize(size, method='pad', pad_mode='border')(base_im, split_idx=1)
    t2 = ToTensor()(img)
    t2 = IntToFloatTensor()(t2)
    t2 = torchvision.transforms.Normalize(*imagenet_stats)(t2)

    if to_cuda:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cpu")

    return t2.to(device)




# Cell
def get_mnet_feature(mnetv2,t_image,to_cuda=False):
    """
    input:
        mnetv2 - our neutered & prepped MobileNet_v2
        t_image - ImageTensor. probaby 3x224x224... but could be a batch
        to_cuda - send to GPU?  default is CPU (to_cuda=False)
    output:
        features - output of mnetv2vector n-1280
    """

    # this is redundant b ut safe
    if to_cuda:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cpu")

    mnet = mnetv2.to(device)
    t_image.to(device)

    if len(t_image.shape)<4:
        t_image = t_image.unsqueeze(0)

    with torch.no_grad():
        features = mnet(t_image)

    return features


# Cell
def query_neighs(q_feat, myneighs, data, root_path, show = True):
    """
    query feature: (vector)
    myneighs:  fit knn object
    data: series or df containing "path"
    root_path:  path to image files
    """
    distance, nn_index = myneighs.kneighbors(q_feat, return_distance=True)
    dist = distance.tolist()[0]

    # fix path to the database...
    neighbors = data.iloc[nn_index.tolist()[0]].copy()
    images = [ PILImage.create(root_path/f) for f in neighbors.path]
    #PILImage.create(btn_upload.data[-1])
    if show:
        for im in images: display(im.to_thumb(IMG_SIZE,IMG_SIZE))

    return images



# Cell
def plot_sneak_neighs(images):
    ''' function to plot matrix of image urls.
        image_urls[:,0] should be the query image

    Args:
        images: list of lists

    return:
        null
        saves image file to directory
    '''
    nrow = len(images)
    ncol = len(images[0])

    fig = plt.figure(figsize = (20, 20))

    num=0
    for row,image_row in enumerate(images):
        for col,img in enumerate(image_row):

            plt.subplot(nrow, ncol, num+1)
            plt.axis('off')
            plt.imshow(img);

            if num%ncol == 0:
                plt.title('Query')

            if col>0:
                plt.title('Neighbor ' + str(col))
            num += 1
    plt.savefig('image_search.png')
    plt.show()



# Cell
def get_umap_embedding(latents):
    reducer = umap.UMAP(random_state=666)
    reducer.fit(latents)
    embedding = reducer.transform(latents)
    assert(np.all(embedding == reducer.embedding_))

    return embedding

# Cell

#define my widgets
btn_run = widgets.Button(description='Find k-nearest neighbors')
out_pl = widgets.Output()
lbl_neighs = widgets.Label()
btn_upload = widgets.FileUpload()

def _load_image(im):
    """input: expects a Path(), but string should work, or a Bytestring

       returns: resized & squared image
    """
    #image = PILImage.create(btn_upload.data[-1])
    image = PILImage.create(im)
    #BUG: pass split_idx=1 to avoid funny business
    image = Resize(IMG_SIZE, method='pad', pad_mode='border')(image, split_idx=1)
    return image

def _prep_image(image,to_cuda=False):
    """input: squared/resized PIL image

        output TensorImage ready to unsqueeze and "embed"
    TODO:  make this a Pipeline?

    """
    t2 = ToTensor()(image)
    t2 = IntToFloatTensor()(t2)
    t2 = torchvision.transforms.Normalize(*imagenet_stats)(t2)

    if to_cuda:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cpu")

    return t2.to(device)




#img = _load_img(im).flip_lr()


def on_click_find_similar(change):
    """ """

    im = btn_upload.data[-1]

    img = _load_image(im)
    tensor_im = _prep_image(img,to_cuda=False)

    feats = get_mnet_feature(mnetv2, tensor_im )
    distance, nn_index = neighs.kneighbors(feats.numpy(), return_distance=True)
    dist = distance.tolist()[0]

    # fix path to the database...
    neighbors = df.iloc[nn_index.tolist()[0]].copy()
    #neighbors.loc[:,'db_path'] = neighbors.loc[:,'path'].astype(str).copy()

    nbr = neighbors.index


    out_pl.clear_output()
    #with out_pl: display(plot_sneak_neighs(img_row[np.newaxis,:]))  # need to convert to pil...


    images = [ PILImage.create(path_images/f) for f in neighbors.path]

    #PILImage.create(btn_upload.data[-1])
    with out_pl:
        display(img.to_thumb(200,200))
        for i in images:
            display(i.to_thumb(100,100))

    lbl_neighs.value = f'distances: {dist}'


btn_run.on_click(on_click_find_similar)




widgets.VBox([widgets.Label('Find your sneaker!'),
      btn_upload, btn_run, out_pl, lbl_neighs])