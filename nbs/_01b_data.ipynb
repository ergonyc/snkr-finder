{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from snkrfinder.imports import *\n",
    "from snkrfinder.core import *\n",
    "\n",
    "### might be fastai wrappers to do this elegantly... (untar_data?)\n",
    "import os\n",
    "from glob import iglob\n",
    "from os.path import join,basename\n",
    "import shutil\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snkrfinder.data b\n",
    "\n",
    "\n",
    "\n",
    "## OVERVIEW: data module- create sub-directories\n",
    "\n",
    "This is a project initiated while an Insight Data Science fellow.  It grew out of my interest in making data driven tools in the fashion/retail space I had most recently been working.   The original over-scoped idea was to make a shoe desighn tool which could quickly develop some initial sneakers based on choosing some examples, and some text descriptors.  Designs are constrained by the \"latent space\" defined (discovered?) by a database of shoe images.  However, given the 3 week sprint allowed for development, I pared the tool down to a simple \"aesthetic\" recommender for sneakers, using the same idea of utilizing an embedding space defined by the database fo shoe images.\n",
    "\n",
    "\n",
    "Feature extractor: \n",
    "    1. embed database into feature space.\n",
    "    2. evaluate/validate by simple logistic regression on classification.\n",
    "\n",
    "\n",
    "\n",
    "### A: load databases / imagefile names to dataframes\n",
    "\n",
    "### B: organize imagefiles into folders according to meta-data (category/source)\n",
    "\n",
    "\n",
    "### C: splitters ... e.g. train, valid, test\n",
    "\n",
    "NOTE:  symbolic link in the nbs directory to enable the module loads in these notebooks.  i.e. `ln -s ../snkrfinder/ snkrfinder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "fastai.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# this should go into a utils or cfg module\n",
    "#DATA_path = Path.home()/'Projects/DATABASE'\n",
    "#DB_path = Path.home()/'Projects/DATABASE'\n",
    "DB_PATH = \"../../Projects/DATABASE\"\n",
    "\n",
    "\n",
    "IMG_SIZE_MD = 160\n",
    "IMG_SIZE = IMG_SIZE_MD\n",
    "\n",
    "ZAPPOS_DF_SIMPLIFIED = \"zappos-50k-simplified_sort\"\n",
    "\n",
    "ZAPPOS_FEATS_ALL = \"zappos-50k-mobilenetv2-features_\"\n",
    "ZAPPOS_FEATS_ALL_SORT = \"zappos-50k-mobilenetv2-features_sort_3\"\n",
    "\n",
    "ZAPPOS_FEATS_MD = f\"mobilenetv2-features_medium\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Train Test Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# UPDATE - JAH, put all data into DATABASE \n",
    "path_images = Path(DB_PATH)/'ut-zap50k-images'\n",
    "\n",
    "data_path = './sneaks/data/' # path of the data\n",
    "train_path = './train'\n",
    "test_path = './test'\n",
    "ldata_path = './data'\n",
    "val_path = './validate'\n",
    "\n",
    "\n",
    "# df.loc[:,'train'] = 1\n",
    "# df.loc[:,'test'] = 0 \n",
    "# df.loc[:,'validate'] = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in iglob(join(data_path,'*')):\n",
    "#for file( in iglob(join(train_path,'*')):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def creat_full_local_categorydirs(df):\n",
    "\n",
    "\n",
    "    # create full set\n",
    "    print('_'*30)\n",
    "    print('Creating full local category set....')\n",
    "    print('_'*30)\n",
    "\n",
    "\n",
    "    for idx in df.index:\n",
    "        save_path = join(ldata_path,df.loc[idx,'CategoryDir'])\n",
    "\n",
    "        #print(save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        img =  join(data_path,df.loc[idx,'path'])\n",
    "        #print(img)\n",
    "        shutil.copy2(img,save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Creating full train set....\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "def create_test_train_directories(df):\n",
    "\n",
    "    # create test set\n",
    "    print('_'*30)\n",
    "    print('Creating full train set....')\n",
    "    print('_'*30)\n",
    "\n",
    "    for idx in df.index:\n",
    "        save_path = join(train_path,df.loc[idx,'CategoryDir'])\n",
    "        #print(save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        img =  join(data_path,df.loc[idx,'path'])\n",
    "        #print(img)\n",
    "        shutil.copy2(img,save_path)\n",
    "    # create test set\n",
    "    print('_'*30)\n",
    "    print('Creating test set....')\n",
    "    print('_'*30)\n",
    "\n",
    "    #instead of looking at the files, lets just use the database\n",
    "\n",
    "    for file in iglob(join(train_path,'*')):\n",
    "    #for file in df.path:\n",
    "        save_path = join(test_path, basename(file))\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        total_imgs = [x for x in iglob(join(file,'*'))]\n",
    "\n",
    "        rand_amt = 2* 0.12 * len(total_imgs)  # select 24% of data from each category as testing + validation set\n",
    "        print(rand_amt)\n",
    "        test_imgs= []\n",
    "        for i in range(int(rand_amt)):\n",
    "            img = random.choice(total_imgs)\n",
    "            if img not in test_imgs:\n",
    "                #print(img)\n",
    "                df.loc[df.Filename == basename(img),'train'] = 0\n",
    "                df.loc[df.Filename == basename(img),'test'] = 1\n",
    "                shutil.move(img,save_path)\n",
    "                test_imgs.append(img)\n",
    "\n",
    "                \n",
    "                # create validation set\n",
    "    print('_'*30)\n",
    "    print('Creating validation set....')\n",
    "    print('_'*30)\n",
    "\n",
    "    #instead of looking at the files, lets just use the database\n",
    "    for file in iglob(join(test_path,'*')):\n",
    "    #for file in df.path:\n",
    "        save_path = join(val_path, basename(file))\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        total_imgs = [x for x in iglob(join(file,'*'))]\n",
    "\n",
    "        rand_amt = 0.5 * len(total_imgs)  # select 50% of data from each category to split evenly between test and validation\n",
    "        print(rand_amt)\n",
    "        test_imgs= []\n",
    "        for i in range(int(rand_amt)):\n",
    "            img = random.choice(total_imgs)\n",
    "            if img not in test_imgs:\n",
    "                #print(img)\n",
    "                df.loc[df.Filename == basename(img),'test'] = 0\n",
    "                df.loc[df.Filename == basename(img),'validation'] = 1\n",
    "                shutil.move(img,save_path)\n",
    "                test_imgs.append(img)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# # create valisation set\n",
    "# print('_'*30)\n",
    "# print('Creating validation set....')\n",
    "# print('_'*30)\n",
    "      \n",
    "# #instead of looking at the files, lets just use the database\n",
    "# for file in iglob(join(train_path,'*')):\n",
    "#             #for file in df.path:\n",
    "#     save_path = join(val_path, basename(file))\n",
    "\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "    \n",
    "#     total_imgs = [x for x in iglob(join(file,'*'))]\n",
    "\n",
    "#     rand_amt = 0.12 * len(total_imgs)  # select 12% of data from each category as testing set\n",
    "#     print(rand_amt)\n",
    "\n",
    "        \n",
    "     ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
